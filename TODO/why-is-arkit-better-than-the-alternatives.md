![1.jpeg](http://upload-images.jianshu.io/upload_images/2208282-0bd3b1a39f23cb7a.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
> * 原文地址：[Why is ARKit better than the alternatives?](https://medium.com/super-ventures-blog/why-is-arkit-better-than-the-alternatives-af8871889d6a)
> * 原文作者：[Matt Miesnieks](https://medium.com/@mattmiesnieks)
> * 译文出自：[掘金翻译计划](https://github.com/xitu/gold-miner)
> * 本文永久链接：[https://github.com/xitu/gold-miner/blob/master/TODO/why-is-arkit-better-than-the-alternatives.md](https://github.com/xitu/gold-miner/blob/master/TODO/why-is-arkit-better-than-the-alternatives.md)
> * 译者：[曹真](https://github.com/LJ147)
> * 校对者：

# ARKit 为什么如此优秀？


苹果公司在近期的 WWDC 上公布的 ARKit 对 AR（Augmented Reality）生态系统产生了巨大影响。开发人员第一次发现，强大而广泛应用的的 AR SDK（iOS 11），恰好适用于他们的 APP。他们不需要再关注标记、初始化、深度摄像机或专有创作工具等。毫无悬念，这引起了一阵开发的热潮（在 Twitter 上关注@madewitharkit 获取最新消息）。

然而，大多数开发人员并不知道 ARKit 如何工作，或者它为什么比其他 SDK 更好。深度探究 ARKit 将帮助我们了解 ARKit 当前的局限性，还需要什么以及为什么，并且有助于预测 Android 和头戴式显示器（VR 或 AR）上的类似功能是否同样可用。

![2.jpeg](http://upload-images.jianshu.io/upload_images/2208282-7e1afd7868fa886e.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

我已经在 AR 领域工作了 9 年，并且在过去创建过与 ARKit 类似的技术（不幸的是那时候硬件设备还不能提高良好的支持）。因此我能够从内部看到这些系统是如何构建的，以及为什么这样构建。

这篇文章主要面向初级开发者，而非专业计算机视觉工程师。我在文中给出的一些简化解释并非百分百科学，但我仍然希望能够帮助读者得到进一步的理解。

# ARKit 基于什么技术构建?

![3.jpeg](http://upload-images.jianshu.io/upload_images/2208282-8344e7588ff55ae5.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/780)

从技术上来讲，ARKit 是一种视觉惯性测距（VIO）系统，兼具一些简单的 2D 平面检测。 VIO 意味着该软件实时跟踪你在空间中的位置（你的 6DOF 姿势），你的姿势在显示器上的每个画面刷新之间重新计算，每秒大约 30 次或更多。这些计算是并行完成两次。通过视觉（相机）系统跟踪你的姿势，通过将现实世界中的一个点与摄像机传感器上的每个像素匹配。

惯性系统（你的加速度计和陀螺仪——合称为惯性测量单元或 IMU ）也跟踪你的姿势。然后，这两个系统的输出通过卡尔曼滤波器进行组合，卡尔曼滤波器确定两个系统中的哪一个提供了你的“真实”位置的最佳估计，并通过 ARKit SDK 发布姿势更新。就像你车内的里程表跟踪车辆所经过的距离，VIO 系统跟踪你的 iPhone 在 6D 空间中行驶的距离。 6D 表示 xyz 运动（平移）的 3 个维度，加上俯仰/偏转/滚动（旋转）的 3 个维度 。



VIO带来的最大优势是 IMU 读数大约是每秒 1000 次，并且基于加速度（用户运动）。航位推算用于测量 IMU 读数之间的设备移动。就像我想要采取一步，猜测这个步骤有多少英寸，你会用推算来估计距离。我稍后会介绍一下这个猜测是否非常准确。惯性系统中的误差会随着时间累积，所以 IMU 帧之间的时间越长，或者惯性系统未从视觉系统获得“重置”的时间越长，跟踪也将与真实值差别越大。

视觉/光学测量以相机帧速率进行，通常为 30fps，并且基于距离（帧之间的场景的变化）。光学系统通常会在距离上累积误差，因此路程越远，误差也就越大。

好消息是，系统能够通过彼此的优势互补消除这些缺陷。


因此，视觉和惯性跟踪系统基于完全不同的测量系统，彼此之间没有相互依赖关系。这意味着相机可以被遮盖，或者可能会看到具有很少光学特征（例如白色墙壁）的场景，这时候惯性系统可以继续完成少量帧数的任务。如果说该设备可以保持静止，这时候视觉系统可以给出比惯性系统更稳定的姿势。卡尔曼滤波器不断选择最佳姿势来实现稳定跟踪。

有趣的是，VIO 系统已经存在了很多年，在行业中也能被很好地理解，并且市场上已经有很多实现。所以苹果使用 VIO 这一事实并不意味着什么。我们需要看看为什么他们的系统如此强大。

ARKit 的第二个主要部分是简单的平面检测。这是必要的，这样你才有平面来放置你的内容，否则它看起来像在空间浮动。这是根据光学系统检测到的特征（你在演示中看到的那些小点）计算出来的，并且算法只需将它们平均化，因为任何 3 个点定义一个平面，如果你花了足够多的时间在这上面，你可以估计真实值。这些点通常被称为另一个混乱的术语：“点云”。这些点都是稀疏点云，用于光学跟踪。稀疏点云使用更少的内存和CPU时间跟踪，并且在惯性系统的支持下，光学系统可以使用更少的点来进行追踪。这是一种不同类型的点云到密集点云，可以看起来接近真实感（注意一些正在处于研究阶段的跟踪器可以使用密集点云进行跟踪，所以这更让人觉得困惑）
 
一点题外话，我看到人们将 ARKit 称为 SLAM，或者使用术语 SLAM 来指代跟踪。为了澄清，将 SLAM 视为一个相当广泛的术语，比如说“多媒体”。跟踪本身是一个更通用的术语，其中测距更加具体，但在实践中它们和 AR 关系更加密切。这可能令人困惑。有很多方法可以实现 SLAM，跟踪只是综合 SLAM 系统的一个组成部分。我认为 ARKit 是一个轻型或简单的 SLAM 系统。Tango 或 Hololens 的 SLAM 系统具有超越距离测量的特征。

# ARKit 的两个奥秘

两个奥秘

* 如何从一个镜头获得3D？
* 如何获得公制尺度（就像那个卷尺演示）？


秘密就在于要很好的去除 IMU 错误（即使是推测非常准确）。当你可以做到这一点时，会发生什么：

要获得 3D，你需要从不同的地方获得 2 个场景的视图，才能对你的位置进行立体计算。这是我们的眼睛在 3D 中看到的，也是为什么一些跟踪器依靠立体相机。如果你有两台摄像机，你可以很方便地知道它们之间的距离，同时帧的捕获也能做到实时。使用一个摄像头，你可以捕获一帧，然后移动，接着捕获第二帧。，你可以通过使用 IMU 计算两帧之间移动的距离，然后进行正常的立体声计算（实际上，你可以超过 2 帧来进行计算，以获得更高的精度）。如果 IMU 足够准确，则可以通过你尝试握住手的微小肌肉运动来检测 2 帧之间的“运动”！所以这看起来像魔术。

要获得公制尺度，系统还依赖于 IMU 的准确的航位推算。 从 IMU 给出的加速度和时间测量值，你可以向后积分以计算速度并再次集成以获得 IMU 帧之间的距离。 数学上并不难。难的事从 IMU 消除错误以获得接近完美的加速度测量。一个微小的错误，会在你移动手机的这段时间以每秒 1000 次的速度累计，最终可能会导致 30％ 或更多的公制尺度误差。事实上，苹果已经把这个错误率下降到 0.01 以内，令人印象深刻。

# Tango & Hololens & Vuforia 等怎么样？

![4.jpeg](http://upload-images.jianshu.io/upload_images/2208282-5ee97430671f9508.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

所以说 Tango 是一个品牌，而不是真正的产品。它包括硬件参考设计（RGB，鱼眼，深度相机和一些CPU / GPU规格）和提供 VIO（运动跟踪），稀疏映射（区域学习）和密集3D重建（深度感知）的软件堆栈。

Hololens 具有完全相同的软件堆栈，但包括一些 ASIC（他们称之为全息处理单元）从 CPU / GPU 卸载处理并节省一些电源。

Vuforia 几乎是一样的，但它是硬件独立的。

以上所有使用相同的 VIO 系统（Tango ＆ ARKit 甚至使用最初由 FlyBy 开发的相同的代码库！）。 Hololens 或 Tango 都不使用深度相机进行跟踪（虽然我相信他们开始在某些方面整合它）。那么为什么 ARKit 这么优秀呢？

答案是 ARKit 并不比 Hololens 好（我甚至认为 Hololens 的追踪器是市场上最好的），但是Hololens 的硬件并没有广泛普及。微软可能会在 Windows 智能手机中搭载 Hololens 跟踪器，但我相信他们这样选择是出于商业原因（它会增加相当多的成本和时间来校准低价出售的手机的传感器，而 ARKit的 MSFT 版本本身不会说服开发人员从 iOS / Android 切换）

谷歌本可以 12 个月前在大众市场的安卓手机中轻松交付 Tango 的 VIO 系统，但他们仍然没有这么做。如果他们这样做了，那么 ARKit 就会看起来像是追上了上了他们的进度，而不是自己（苹果公司）做出了重大突破。我相信（没有百分百的把握），这是因为他们不想为每个 OEM 都进行独特的传感器校准过程，每个 OEM 厂商的版本都不如其他 OEM，而谷歌不希望只是少数几家巨大的 OEM（三星，华为等）。相反，他们告诉 OEM 厂商：“这是硬件的参考设计，要么接受要么出局”。 当然这并不简单，但这正是 OEM 厂商给予的反馈意见。随着安卓已经将智能手机硬件商品化，相机和传感器堆栈是最后的差异化领域之一，因此 OEM 厂商无法融合谷歌的需求。Tango 还强制说，深度相机是包装的一部分，这增加了 BOM 的手机成本（和咀嚼电池），这是 OEM 厂商拒绝的另一个原因！由于 ARKit 的出现，世界已经改变了。看看最后是 OEM 找到了 Tango 的替代系统，还是谷歌对硬件参考设计作出让步（从而控制平台）。

所以 ARKit 更好的原因是因为苹果公司有能力将 VIO 算法紧密耦合到传感器上，花很多时间进行校准，以消除姿态计算中的错误/不确定性。


值得注意的是，大型 OEM 系统有很多替代方案。 有很多学术追踪器（ORB Slam是一个很好的选择，OpenCV 有一些选项等），但它们几乎都是光学（单 RGB 或立体声，和/或深度相机，有些使用稀疏的地图、有些是密集、有些是深度、有些是地图以及其余的使用传感器的半直接数据）。有许多创业公司正在研究跟踪系统，增强像素这一点表现良好，但是最终任何 VIO 系统都需要硬件建模和校准来进行竞争。

# 我是一名开发者，我应该使用什么以及为什么？


![5.jpeg](http://upload-images.jianshu.io/upload_images/2208282-25fb7b17ba300e7c.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
 
首先应用 ARKit 来实现你的 idea。它非常好用而且你可能已经有一个支持它的手机。逐渐了解设计和开发一个实际应用程序之间的巨大差别，这个阶段你不需要控制场景和每一个像素。
 
然后开始学习 Tango 或 Hololens 。这时候要深入了解当你的内容与不受控场景的 3D 结构进行交互发生了什么。


这是一个**非常陡**的学习曲线。比从网络到移动或从移动到 VR 更大。你需要彻底重新思考应用程序的工作原理以及 UX 或用例是否有意义。我看到很多建立在 ARKit 上的示例，也看到 4 年前建立在 Vuforia 和在 Layar上的示例。开发人员正在重新学习相同的课程，但规模很大。几年来，我看到了几乎所有类型的 AR Apps 的示例，我很乐意提供反馈和支持。欢迎随时联系我。

我鼓励开发人员不要害怕开发新奇的应用程序。应用程序是智能手机的第一击。
 

**这是一个非常小的世界。 但没多少人能很好地构建这些系统。**

![6.jpeg](http://upload-images.jianshu.io/upload_images/2208282-b8e303ac43963d6c.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如何建立高质量的追踪者的一个令人着迷和低估的方面是，世界上只有少数人可以搭建它们。这些工程师的相互关联的职业生涯使得将单眼 VIO 融合的最佳系统成为移动跟踪的“解决方案”。现在没有其他途径提供这样的 UX。

VIO 于二十世纪中叶由波士顿军工/工业供应商 Intersense 构建。联合创始人之一的 Leonid Naimark 是 2011 年 Dekko 的首席科学家。由于传感器的限制， VIO 被证实无法在 iPad 2 上运行。Leonid 继续完成军事合同，而 Dekko 的首席技术官 Pierre Georgel 现在则是谷歌 Daydream 团队的高级工程师。 Ogmento 由我的 Super Ventures 合作伙伴 Ori Inbar 创立。 Ogmento 成为 FlyBy，并且在那里的团队成功地构建了一个使用附加鱼眼相机的 iOS 上的VIO系统。该代码库已经授权给谷歌，成为 Tango 的 VIO 系统。苹果后来买了 FlyBy，同样的代码库是 ARKit VIO 的核心。 FlyBy 的首席技术官继续为 Daqri 建立了跟踪器，现在是一家自主机器人公司，与前任首席科学家 Zoox合作。第一个移动 SLAM 系统是在 2007 年左右在牛津由 George Klein 和在特斯拉建立自主系统的 David Nister等合作开发的，他们接下来为 Hololens 建立 VIO 系统，以及。乔治的博士生Gerhard Reitmayr 领导了 Vuforia 的 VIO 系统的发展。 牛津大学剑桥皇家学院研究团队的主要成员开发了 Kinect 跟踪系统，现在在 Oculus 和 Magic Leap 领导跟踪团队。

有趣的是，我不知道在这个领域工作的哪些 AR 创业公司是由这个小型人才库的工程人才领导的，而来自机器人技术或其他类型的计算机视觉背景的创始人还没有能够展示在广泛工作的系统环境范围。

稍后我会谈谈当前研究科学家正在开展的工作。 提示：不是VIO。

# 数据代表着性能

![7.jpeg](http://upload-images.jianshu.io/upload_images/2208282-92ef7bff02f1821c.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
AR 系统从来没有奏效或不奏效。把一项事情做的应用的足够宽泛总是一件很好的事情。“获得更好的成绩”总是意味着将统计数据朝着有利于你的方向推进。

基于以上原因，不要信任 AR App 的演示示例，特别是那些在 YouTube 上展现惊人效果的。因为在人为控制或预布置的环境中表现良好，不代表能够很好的应用到实际中，这两者之间仍然存在很大的差距。那些演示的效果不能在智能手机或 VR 应用程序上复现（例如，如果松弛成功或根据你的相机碰巧指向的位置或者你如何碰巧移动手腕时无法使用），所以观众往往会被愚弄。

这是一个具体的技术示例关于为什么统计数据最终确定系统的运行情况。

在这个图像中，我们有一个代表相机中的数字图像传感器的网格。每个框都是一个像素。为了跟踪稳定，每个像素应该匹配现实世界中的一个对应点（假设设备完全静止）。然而，第二个图像显示光子不是那么匹配，并且各种光线的强度会下降到任何他们想要的地方，每个像素只是光子的总数。场景中的光线变化（云通过太阳，荧光灯闪烁等）改变击中传感器的光子的组成，现在传感器具有与现实世界点对应的不同像素。就视觉跟踪系统而言，你已经移动了！这就是为什么当你看到各种 ARKit 演示文稿中的点数闪烁时，系统必须确定哪些点是“可靠”的。那么它必须从这些点进行三角测量来计算姿势，对计算结果进行均值处理，以获得对实际姿势的最佳估计。因此，任何能够确保统计错误从此过程中移除的工作都将确保一个更健壮的系统。这需要相机硬件堆栈（多个透镜和涂层，快门和图像传感器规格等）以及 IMU 硬件和软件算法之间的紧密集成和校准。

# 集成硬件和软件

![8.jpeg](http://upload-images.jianshu.io/upload_images/2208282-c851dc3bc8e2c129.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

有趣的是，VIO 不难正常运转。现如今存在很多已发布的算法以及对应的算法实现。但是让 VIO 运作的**非常**良好却很难。这意味着惯性和光学系统几乎实时融合到立体地图上，并且可以用低的单位数精度来确定度量标度。我们在 Dekko 建立的实施要求用户一开始做出特定的姿势，然后将手机前后移动约 30 秒才能聚合。建立一个伟大的惯性跟踪系统需要有经验的工程师。不幸的是，地球上只有大约 20 名工程师具有必要的技能和经验，而且其中大多数工程师正在建造巡航导弹跟踪系统，或者是火星漫游者导航系统等。

所以即使你可以接触到这些人之一，所有的事情仍然取决于硬件和软件是否处于锁定状态，以最大限度地减少错误。在这个核心上，这意味着可以通过软件准确建模 IMU，完全访问整个摄像机堆栈以及堆叠中每个组件的详细规格，最重要的是 IMU 和摄像头需要非常精确地进行时钟同步。系统需要准确知道哪个 IMU 读取对应于帧捕获的开始，哪个到底是什么。

这对于两个系统的关联至关重要，直到最近才是不可能的，因为硬件 OEM 厂商没有理由投资于此。这就是 Dekko 基于 iPad 2 的系统花费了很长时间才能融合的原因。第一个 Tango Peanut 手机是准确时钟同步所有内容的第一个设备，并且是第一个提供良好跟踪的消费者手机。今天，Qualcom 等的芯片系统都有一个同步的传感器集线器，用于所有组件的使用，这意味着 VIO 在大多数当前设备上可行，并配有合适的传感器校准。


由于硬件和软件的紧密依赖，软件开发人员几乎不可能在没有 OEM 的深入支持的情况下构建一个伟大的系统来构建合适的硬件。 谷歌投入了大量资金，让一些 OEM 厂商支持 Tango 高清规格。 MSFT，Magic Leap 等正在建立自己的硬件。苹果之所以能够在 ARKit 上取得如此巨大的成功就是因为它已经能够实现两者（软硬件）的协调统一。
 
# 光学校准

![9.jpeg](http://upload-images.jianshu.io/upload_images/2208282-7b80cbecfb41ca04.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
为了使软件能够精确校准摄像机传感器上的像素是否现实世界中的点匹配，摄像机系统需要精确校准。有两种类型的校准：

## 几何校准
使用相机的针孔模型来校正镜头的视场和镜头的镜筒效果。由于透镜的形状，所有的图像都会变形。大多数软件开发人员可以在没有 OEM 输入的情况下使用棋盘基本相机规格进行此步骤。

## 光度校准
这是更多的参与，通常需要 OEM 参与图像传感器本身的细节，内部透镜等的任何涂层。此校准处理颜色和强度映射。例如，远摄恒星拍摄的望远镜连接的摄像机需要知道传感器上的像素上的光强度的轻微变化确实是星形，还是传感器或透镜中的像差。跟踪器的结果比传感器上的像素匹配真实世界点的确定性更高，因此光学跟踪更加鲁棒，误差更少。


在上面的幻灯片中，落入图像传感器上的像素的桶中的各种 RGB 光子的图片说明了问题。来自现实世界中的一点的光通常落在几个像素的边界上，并且这些像素中的每一个将平均所有击中它的光子的强度。用户运动或场景阴影的微小变化，或闪烁的荧光灯会改变哪个像素最能代表现实世界的点。这是所有这些光学校准都尽可能地消除的错误。

# 惯性校准

![10.jpeg](http://upload-images.jianshu.io/upload_images/2208282-b73f20505e3fca22.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

在考虑 IMU 时，记住测量加速度而非距离或速度至关重要。 IMU 中的错误随着时间的推移积累速度非常快！校准和建模的目标是确保距离的测量（双加速度加速度）对 X 秒的精度足够高。理想情况下，这是一个足够长的时期，以避免当用户覆盖镜头或场景中发生的其他事情时，相机在几帧内失去跟踪目标。

使用 IMU 测量距离称为推算。这基本上是一个猜测，但是通过对 IMU 的行为进行建模，找出所有的方法来积累错误，然后编写过滤器来减轻这些错误。

想象一下，如果你被要求走一步然后猜测你踩了几英寸。单步猜测会有很高的误差。如果你反复采取数千步骤，从而衡量每一个，并学习你迈出哪一只脚、地板、你穿的鞋子、你的移动速度、你的疲劳程度等等，那么猜测最终会变得非常准确。这是基本的 IMU 校准和建模原理。

有很多错误来源。 机器人臂通常用于以完全相同的方式重复地移动设备，并且捕获来自 IMU 的输出并写入滤波器，直到来自 IMU 的输出与来自机器人臂的真实运动精确匹配。谷歌和微软甚至在 ISS 或“零重力航班”上发送了微型重力，以消除额外的错误。


![这只是其中一些必须从跟踪中识别的错误，如图中的RGB线](http://upload-images.jianshu.io/upload_images/2208282-24e1b43a50f2da56.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
> 这只是其中一些必须从跟踪中识别的错误，如图中的RGB线

这听起来更难准确。 OEM也是一个 PITA，必须对其组合中的所有设备进行此过程，即使这样，许多设备也可能会有不同的 IMU（例如，Galaxy 7 可能拥有来自 Invensense 或 Bosch 的 IMU，当然也可以为 博世不适用于Invensense等）。 这是苹果相对于 Android OEM 的优势的另一个领域。

# 追踪的未来 


![12.jpeg](http://upload-images.jianshu.io/upload_images/2208282-e2efe5efd084f9e9.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

如果 VIO 是现在所呈现的样子，那么接下来会发生什么？它会使 ARKit 变得多余吗？令人惊讶的是，VIO 仍然是追踪数百米范围内最好的方法（对于更长的时间，系统将需要使用融合到系统中的 GPS 组合重新定位加上某种地标识别）。

这样做的原因是，即使其他光学系统只能像 VIO 一样准确，他们仍然需要更多（ GPU 或摄像头）电源，这在 HMD 中非常重要。单眼 VIO 是最准确、最低功耗以及最低成本的解决方案。

深入学习真正对研究界有追踪的影响。到目前为止，基于深度学习的系统大约是 10％ 的错误率，而顶级的 VIO 系统是前者错误率的一部分，但它们正在相互追赶，并将在未来真正有助于户外重定位。

深度相机可以通过几种方式帮助 VIO 系统。对于低特征场景，准确测量地面真值、度量标度和边缘跟踪是最大的好处。它们非常耗费能源，因此以非常低的帧速率，并且在帧之间使用 VIO 运行它们很有意义。它们不能在户外运行因为红外线散射从阳光中洗出红外线从深度相机。它们的工作范围也取决于它们的功耗，这意味着在手机上只有很短的工作距离。它们在 BOM 成本方面也很昂贵，因此 OEM 将避免使用大容量手机。

立体声 RGB 或鱼眼镜头有助于能够看到更大的场景（因此潜在的更多的光学特征，例如普通镜头可能会看到白色的墙壁，但鱼眼可以看到图案的天花板和地毯 - Tango 和 Hololens 使用这个方法），并且可能获得比 VIO 更低计算成本的深度信息。由于手机（甚至 HMD ）上的立体声摄像机靠近在一起，它们的精确范围对于深度计算是非常有限的（相隔两厘米的相机可以精确到高达几米的深度）。

管道（pipeline）最有趣的事情是支持在更大的地区进行跟踪，尤其是户外。 在这一点上，跟踪 AR 和跟踪自驾车几乎没有区别，除了 AR 系统使用较少的传感器和较低的功率。 因为任何设备最终都将耗尽内存如果试图映射大面积，所以需要云支持服务，因此谷歌最近宣布了 Tango 视觉定位服务。 我们将在未来几个月内看到更多。 这也是每个人都关心 3D 地图的原因。

# AR和计算机视觉的未来 


![13.jpeg](http://upload-images.jianshu.io/upload_images/2208282-adc092edb280c512.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)
6D 位置跟踪将在 12-18 个月内完全商品化，涵盖所有设备。待解决的问题还有：


3D 重建（Hololens 术语中称作空间映射，Tango 术语中称作深度感知）。这项技术能够找出场景中真实物体的形状或结构，同时允许虚拟内容与现实世界碰撞并隐藏在后面（遮挡）。这也是使人们混淆的功能，因为他们认为这意味着 AR 现在是“混合”的现实。 3D 重建通过从场景（现在使用深度相机）捕获点云，然后将其转换为网格，并将“不可见”网格馈送到 Unity （与真实世界坐标）并将真实世界网格准确放置在在相机中出现的真实世界的顶部。这意味着虚拟内容似乎与现实世界互动。注意 ARKit 现在通过检测 2D 平面来执行 2D 版本。这是所需的最小值。没有一个真实的平面，Unity 的实体在平面上立不住，并且会浮动。

![Magic Leap 的示例显示机器人藏在桌子腿后面。 不知道桌子腿是实时重建，还是他们预先对其进行建模，然后手动把它放在真实的桌子上。](http://upload-images.jianshu.io/upload_images/2208282-fc89ea515b69833d.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

Magic Leap 的示例显示机器人藏在桌子腿后面。 不知道桌子腿是实时重建，还是他们预先对其进行建模，然后手动把它放在真实的桌子上。

上述深度摄像机的所有问题仍然适用于此，这就是为什么它不不能做到广泛应用。 （科学界）正在进行积极的研究，以支持使用单个 RGB 相机的实时照片级3D重建。 在产品中看到的时间大约是 12-18 个月。 这就是我认为“真正的”商用级别 AR HMD 仍然很遥远的一个主要原因。

![2012 年Dekko  基于 iPad 2 的 3D 重构系统。我们不得不显示网格，否则用户不相信他们看到的内容（系统可以理解现实世界）。（ 越野车刚刚完成了跳跃，部分隐藏在纸巾盒后面）。](http://upload-images.jianshu.io/upload_images/2208282-a5aca5719e5fe7b0.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

>  2012 年Dekko  基于 iPad 2 的 3D 重构系统。我们不得不显示网格，否则用户不相信他们看到的内容（系统可以理解现实世界）。（ 越野车刚刚完成了跳跃，部分隐藏在纸巾盒后面）。

在 3D 重建之后，有关于语义上理解 3D 场景的许多有趣的研究。 几乎所有惊人的计算机视觉深入学习，你已经看到使用 2D 图像（常规照片），但对于 AR（和汽车，无人机等），我们需要对 3D 世界的语义理解。 像 ScanNet 这样的新举措将会帮助很多，类似于 ImageNet帮助 2D 语义的方式。

![场景 3D 语义分割的一个例子。 原图在底部，上面是 3D 模型（可能是由立体相机或 LIDAR 构建的），而最重要的是通过深度学习进行分割，因此我们可以从道路上识别出人行道。 这对于 Pokemon Go 也是有用的，所以 Pokemon Go 没有放在繁忙的路上](http://upload-images.jianshu.io/upload_images/2208282-a9e803b25b8987fd.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 场景 3D 语义分割的一个例子。 原图在底部，上面是 3D 模型（可能是由立体相机或 LIDAR 构建的），而最重要的是通过深度学习进行分割，因此我们可以从道路上识别出人行道。 这对于 Pokemon Go 也是有用的，所以 Pokemon Go 没有放在繁忙的路上。

那么我们就需要弄清楚，如何将这些惊人的技术扩展到能够实时支持多个并发用户。

![随着 3D 重建越来越大规模，我们需要了解如何在云中托管它们，让多个用户共享（并扩展）模型。
](http://upload-images.jianshu.io/upload_images/2208282-1f5403d4c11497e4.jpeg?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240)

> 随着 3D 重建越来越大规模，我们需要了解如何在云中托管它们，让多个用户共享（并扩展）模型。

# AR 其它方面的未来 

深度探讨所有的方面超出了这篇文章的范畴，但技术堆栈仍然有很多任务要进一步发展：

* 光学：视场，眼框尺寸，分辨率，亮度，焦点深度，聚光度都需要解决。我认为在我们看到最终的消费者解决方案之前，我们将会看到几个“过渡”的 HMD 设计 —— 只有有限的功能集，每次只“解决”一个问题（如社交信号、跟踪、企业使用情况等）。


* 渲染：使虚拟内容与现实世界相一致。确定真实的光源并将其实际匹配，使阴影和纹理看起来正确等等。这基本上是好莱坞 SFX 多年来一直在做的（所以复仇者看起来很真实），但现实中，只有一部手机的话，你是无法控制真实的世界照明或背景等的。 

* 输入：这一步还有很长的路要走。研究表明，截止目前多模式输入系统变现最佳。有传言表明苹果公司即将搭载该系统。多模式只是意味着 AI 同时考虑到各种输入“模式”（手势，语音，计算机视觉，触摸，眼睛跟踪等），以最大化地了解用户的意图。

* GUI 和应用程序：没有像我们想象的AR那样的app”这样的东西。我们只想看看我们的 Sonos，并且几乎在设备上显示控件。我们不想选择一个小的Sonos按钮。还是我们？在我们的视野（像战斗机飞行员HUD）vs附加到真实的单词对象上，我们想要什么控制。没有人有什么真正的想法如何发挥，但它不会是一个4 x 6的图标网格。

* 待解决的社会性问题。 目前只有 Apple 和 Snap 知道如何销售时尚的产品。这可能比以上所有技术问题要更难解决。
 
 感谢你看到这里！有任何关于之后文章的问题、建议或要求都欢迎与我联系。

 
> [掘金翻译计划](https://github.com/xitu/gold-miner) 是一个翻译优质互联网技术文章的社区，文章来源为 [掘金](https://juejin.im) 上的英文分享文章。内容覆盖 [Android](https://github.com/xitu/gold-miner#android)、[iOS](https://github.com/xitu/gold-miner#ios)、[React](https://github.com/xitu/gold-miner#react)、[前端](https://github.com/xitu/gold-miner#前端)、[后端](https://github.com/xitu/gold-miner#后端)、[产品](https://github.com/xitu/gold-miner#产品)、[设计](https://github.com/xitu/gold-miner#设计) 等领域，想要查看更多优质译文请持续关注 [掘金翻译计划](https://github.com/xitu/gold-miner)、[官方微博](http://weibo.com/juejinfanyi)、[知乎专栏](https://zhuanlan.zhihu.com/juejinfanyi)。

