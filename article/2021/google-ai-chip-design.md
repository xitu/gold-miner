> * 原文地址：[Google's Apollo AI for Chip Design Improves Deep Learning Performance by 25%](https://www.infoq.com/news/2021/03/google-ai-chip-design)
> * 原文作者：[Anthony Alford]()
> * 译文出自：[掘金翻译计划](https://www.infoq.com/profile/Anthony-Alford/)
> * 本文永久链接：[https://github.com/xitu/gold-miner/blob/master/article/2021/google-ai-chip-design.md](https://github.com/xitu/gold-miner/blob/master/article/2021/google-ai-chip-design.md)
> * 译者：[PingHGao](https://github.com/PingHGao)
> * 校对者：

# Google 的 Apollo 芯片设计人工智能框架将深度学习芯片的性能提高了 25％

[Google Research](https://research.google/) 的科学家公布了一种用于优化人工智能加速器芯片设计的新框架 [APOLLO](https://arxiv.org/abs/2102.01723)。APOLLO 使用革命性的算法选择芯片参数，以最小的芯片面积最大程度地减少深度学习推理延迟。在阿波罗的帮助下，研究人员找到了比那些通过基线算法选择的设计快 24.6% 的设计方案。

研究科学家 Amir Yazdanbakhsh 在最近的博客文章中对该系统进行了概述。APOLLO 搜索一组硬件参数，例如内存大小，I/O 带宽和处理器单元，为给定的深度学习模型提供最佳的推理性能。通过使用革命性的算法和迁移学习，APOLLO 可以有效地探索参数空间，从而减少设计的总体时间和成本。Yazdanbakhsh 认为：

> We believe that this research is an exciting path forward to further explore ML-driven techniques for architecture design and co-optimization (e.g., compiler, mapping, and scheduling) across the computing stack to invent efficient accelerators with new capabilities for the next generation of applications.我们相信，这项研究是进一步探索机器学习驱动技术以在整个计算堆栈中进行体系结构设计和协同优化（例如，编译器，映射和调度）以发明出具有下一代新功能的高效加速器的令人振奋的道路。应用程序。

Deep-learning models have been developed for a wide variety of problems, from computer vision (CV) to natural language processing (NLP). However, these models often require large amounts of compute and memory resources at inference time, straining the hardware constraints of edge and mobile devices. Custom accelerator hardware, such as [Edge TPUs](https://www.infoq.com/news/2020/12/google-coral-ai-iot/), can improve model inference latency, but often require modifications to the model, such as parameter quantization or [model pruning](https://www.infoq.com/presentations/tensorflow-lite/). Some researchers, including [a team at Google](https://arxiv.org/abs/2003.02838), have proposed [using AutoML](https://arxiv.org/abs/1812.00332) to design high-performance models targeted for specific accelerator hardware.深度学习模型针对各种问题而开发，从计算机视觉（CV）到自然语言处理（NLP）。但是，这些模型在推理时通常需要大量的计算和内存资源，从而使边缘和移动设备的硬件约束更加紧张。定制加速器硬件（例如Edge TPU）可以改善模型推理延迟，但通常需要对模型进行修改，例如参数量化或模型修剪。包括Google团队在内的一些研究人员已经建议使用AutoML设计针对特定加速器硬件的高性能模型。

The APOLLO team's strategy, by contrast, is to customize the accelerator hardware to optimize performance for a given deep-learning model. The accelerator is based on a 2D array of processing elements (PEs), each of which contains a number of [single instruction multiple data](https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data) (SIMD) cores. This basic pattern can be customized by choosing values for several different parameters, including the size of the PE array, the number of cores per PE, and the amount of memory per core. Overall, there are nearly 500M parameter combinations in the design space. Because a proposed accelerator design must be simulated in software, evaluating its performance on a deep-learning model is time and compute intensive.相比之下，APOLLO团队的策略是定制加速器硬件，以优化给定深度学习模型的性能。加速器基于处理元件（PE）的2D阵列，每个处理元件包含多个单指令多数据（SIMD）内核。可以通过为几个不同的参数选择值来定制此基本模式，这些参数包括PE阵列的大小，每个PE的内核数以及每个内核的内存量。总体而言，设计空间中有近500M个参数组合。由于建议的加速器设计必须在软件中进行仿真，因此在深度学习模型上评估其性能既耗时又需要大量计算。

APOLLO builds on Google's internal [Vizier](https://research.google/pubs/pub46180/) "black box" optimization tool, and Vizier's optimization Bayesian method is used as a baseline comparison for evaluating APOLLO's performance. The APOLLO framework supports several optimization strategies, including random search, [model-based optimization](https://research.google/pubs/pub49138/), evolutionary search, and an ensemble method called population-based black-box optimization (P3BO). The Google team performed several experiments, searching for optimal accelerator parameters for a set of CV models, including [MobileNetV2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html) and [MobileNetEdge](https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html), for three different chip-area constraints. They found that the P3BO algorithm produced the best designs and its performance improved compared to Vizier as available chip area decreased. Compared to a manually-guided exhaustive or "brute-force" search, P3BO found a better configuration while performing 36% fewer search evaluations.APOLLO建立在Google内部的Vizier “黑匣子”优化工具之上，并且Vizier的优化贝叶斯方法用作评估APOLLO性能的基准比较。APOLLO框架支持多种优化策略，包括随机搜索，基于模型的优化，进化搜索以及称为基于种群的黑盒优化（P3BO）的整体方法。该谷歌团队进行了几个实验，寻找最佳油门参数集CV模型，包括MobileNetV2和MobileNetEdge，用于三个不同的芯片区域约束。他们发现P3BO算法产生了最好的设计，并且与Vizier相比，随着可用芯片面积的减少，其性能得到了改善。与手动引导的穷举搜索或“蛮力搜索”相比，P3BO发现了更好的配置，同时执行的搜索评估减少了36％。

The design of accelerator hardware for improving AI inference is an active research area. Apple's new [M1 processor](https://www.infoq.com/news/2020/11/apple-tensorflow-acceleration/) includes a neural engine designed to speed up AI computations. Stanford researchers recently published an article in Nature describing a system called [Illusion](https://ee.stanford.edu/news/research-news/01-19-2021/subhasish-mitra-hs-philip-wong-and-mary-wootters-system-can-run-ai) that uses a network of smaller chips to emulate a single larger accelerator. At Google, scientists have also published work on [optimizing chip floorplanning](https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html), to find the best placement of integrated-circuit components on the physical chip.用于改善AI推理的加速器硬件设计是一个活跃的研究领域。苹果新的M1处理器包括一个神经引擎，旨在加速AI计算。斯坦福大学的研究人员最近在《自然》杂志上发表了一篇文章，描述了一个名为Illusion的系统，该系统使用较小芯片的网络来模拟单个较大的加速器。在Google，科学家还发表了有关优化芯片布局的工作，以寻找集成电路组件在物理芯片上的最佳放置。

> 如果发现译文存在错误或其他需要改进的地方，欢迎到 [掘金翻译计划](https://github.com/xitu/gold-miner) 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 **本文永久链接** 即为本文在 GitHub 上的 MarkDown 链接。

---

> [掘金翻译计划](https://github.com/xitu/gold-miner) 是一个翻译优质互联网技术文章的社区，文章来源为 [掘金](https://juejin.im) 上的英文分享文章。内容覆盖 [Android](https://github.com/xitu/gold-miner#android)、[iOS](https://github.com/xitu/gold-miner#ios)、[前端](https://github.com/xitu/gold-miner#前端)、[后端](https://github.com/xitu/gold-miner#后端)、[区块链](https://github.com/xitu/gold-miner#区块链)、[产品](https://github.com/xitu/gold-miner#产品)、[设计](https://github.com/xitu/gold-miner#设计)、[人工智能](https://github.com/xitu/gold-miner#人工智能)等领域，想要查看更多优质译文请持续关注 [掘金翻译计划](https://github.com/xitu/gold-miner)、[官方微博](http://weibo.com/juejinfanyi)、[知乎专栏](https://zhuanlan.zhihu.com/juejinfanyi)。
