> * 原文地址：[Google's Apollo AI for Chip Design Improves Deep Learning Performance by 25%](https://www.infoq.com/news/2021/03/google-ai-chip-design)
> * 原文作者：[Anthony Alford]()
> * 译文出自：[掘金翻译计划](https://www.infoq.com/profile/Anthony-Alford/)
> * 本文永久链接：[https://github.com/xitu/gold-miner/blob/master/article/2021/google-ai-chip-design.md](https://github.com/xitu/gold-miner/blob/master/article/2021/google-ai-chip-design.md)
> * 译者：
> * 校对者：

# Google's Apollo AI for Chip Design Improves Deep Learning Performance by 25%

Scientists at [Google Research](https://research.google/) have announced [APOLLO](https://arxiv.org/abs/2102.01723), a framework for optimizing AI accelerator chip designs. APOLLO uses evolutionary algorithms to select chip parameters that minimize deep-learning inference latency while also minimizing chip area. Using APOLLO, researchers found designs that achieved 24.6% speedup over those chosen by a baseline algorithm.

Research Scientist Amir Yazdanbakhsh gave a [high level overview](https://ai.googleblog.com/2021/02/machine-learning-for-computer.html) of the system in a recent blog post. APOLLO searches for a set of hardware parameters, such memory size, I/O bandwidth, and processor units, that provides the best inference performance for a given deep-learning model. By using evolutionary algorithms and transfer learning, APOLLO can efficiently explore the space of parameters, reducing the overall time and cost of producing the design. According to Yazdanbakhsh,

> We believe that this research is an exciting path forward to further explore ML-driven techniques for architecture design and co-optimization (e.g., compiler, mapping, and scheduling) across the computing stack to invent efficient accelerators with new capabilities for the next generation of applications.

Deep-learning models have been developed for a wide variety of problems, from computer vision (CV) to natural language processing (NLP). However, these models often require large amounts of compute and memory resources at inference time, straining the hardware constraints of edge and mobile devices. Custom accelerator hardware, such as [Edge TPUs](https://www.infoq.com/news/2020/12/google-coral-ai-iot/), can improve model inference latency, but often require modifications to the model, such as parameter quantization or [model pruning](https://www.infoq.com/presentations/tensorflow-lite/). Some researchers, including [a team at Google](https://arxiv.org/abs/2003.02838), have proposed [using AutoML](https://arxiv.org/abs/1812.00332) to design high-performance models targeted for specific accelerator hardware.

The APOLLO team's strategy, by contrast, is to customize the accelerator hardware to optimize performance for a given deep-learning model. The accelerator is based on a 2D array of processing elements (PEs), each of which contains a number of [single instruction multiple data](https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data) (SIMD) cores. This basic pattern can be customized by choosing values for several different parameters, including the size of the PE array, the number of cores per PE, and the amount of memory per core. Overall, there are nearly 500M parameter combinations in the design space. Because a proposed accelerator design must be simulated in software, evaluating its performance on a deep-learning model is time and compute intensive.

APOLLO builds on Google's internal [Vizier](https://research.google/pubs/pub46180/) "black box" optimization tool, and Vizier's optimization Bayesian method is used as a baseline comparison for evaluating APOLLO's performance. The APOLLO framework supports several optimization strategies, including random search, [model-based optimization](https://research.google/pubs/pub49138/), evolutionary search, and an ensemble method called population-based black-box optimization (P3BO). The Google team performed several experiments, searching for optimal accelerator parameters for a set of CV models, including [MobileNetV2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html) and [MobileNetEdge](https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html), for three different chip-area constraints. They found that the P3BO algorithm produced the best designs and its performance improved compared to Vizier as available chip area decreased. Compared to a manually-guided exhaustive or "brute-force" search, P3BO found a better configuration while performing 36% fewer search evaluations.

The design of accelerator hardware for improving AI inference is an active research area. Apple's new [M1 processor](https://www.infoq.com/news/2020/11/apple-tensorflow-acceleration/) includes a neural engine designed to speed up AI computations. Stanford researchers recently published an article in Nature describing a system called [Illusion](https://ee.stanford.edu/news/research-news/01-19-2021/subhasish-mitra-hs-philip-wong-and-mary-wootters-system-can-run-ai) that uses a network of smaller chips to emulate a single larger accelerator. At Google, scientists have also published work on [optimizing chip floorplanning](https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html), to find the best placement of integrated-circuit components on the physical chip.

Scientists at [Google Research](https://research.google/) have announced [APOLLO](https://arxiv.org/abs/2102.01723), a framework for optimizing AI accelerator chip designs. APOLLO uses evolutionary algorithms to select chip parameters that minimize deep-learning inference latency while also minimizing chip area. Using APOLLO, researchers found designs that achieved 24.6% speedup over those chosen by a baseline algorithm.

Research Scientist Amir Yazdanbakhsh gave a [high level overview](https://ai.googleblog.com/2021/02/machine-learning-for-computer.html) of the system in a recent blog post. APOLLO searches for a set of hardware parameters, such memory size, I/O bandwidth, and processor units, that provides the best inference performance for a given deep-learning model. By using evolutionary algorithms and transfer learning, APOLLO can efficiently explore the space of parameters, reducing the overall time and cost of producing the design. According to Yazdanbakhsh,

> We believe that this research is an exciting path forward to further explore ML-driven techniques for architecture design and co-optimization (e.g., compiler, mapping, and scheduling) across the computing stack to invent efficient accelerators with new capabilities for the next generation of applications.

Deep-learning models have been developed for a wide variety of problems, from computer vision (CV) to natural language processing (NLP). However, these models often require large amounts of compute and memory resources at inference time, straining the hardware constraints of edge and mobile devices. Custom accelerator hardware, such as [Edge TPUs](https://www.infoq.com/news/2020/12/google-coral-ai-iot/), can improve model inference latency, but often require modifications to the model, such as parameter quantization or [model pruning](https://www.infoq.com/presentations/tensorflow-lite/). Some researchers, including [a team at Google](https://arxiv.org/abs/2003.02838), have proposed [using AutoML](https://arxiv.org/abs/1812.00332) to design high-performance models targeted for specific accelerator hardware.

The APOLLO team's strategy, by contrast, is to customize the accelerator hardware to optimize performance for a given deep-learning model. The accelerator is based on a 2D array of processing elements (PEs), each of which contains a number of [single instruction multiple data](https://www.sciencedirect.com/topics/computer-science/single-instruction-multiple-data) (SIMD) cores. This basic pattern can be customized by choosing values for several different parameters, including the size of the PE array, the number of cores per PE, and the amount of memory per core. Overall, there are nearly 500M parameter combinations in the design space. Because a proposed accelerator design must be simulated in software, evaluating its performance on a deep-learning model is time and compute intensive.

APOLLO builds on Google's internal [Vizier](https://research.google/pubs/pub46180/) "black box" optimization tool, and Vizier's optimization Bayesian method is used as a baseline comparison for evaluating APOLLO's performance. The APOLLO framework supports several optimization strategies, including random search, [model-based optimization](https://research.google/pubs/pub49138/), evolutionary search, and an ensemble method called population-based black-box optimization (P3BO). The Google team performed several experiments, searching for optimal accelerator parameters for a set of CV models, including [MobileNetV2](https://ai.googleblog.com/2018/04/mobilenetv2-next-generation-of-on.html) and [MobileNetEdge](https://ai.googleblog.com/2019/11/introducing-next-generation-on-device.html), for three different chip-area constraints. They found that the P3BO algorithm produced the best designs and its performance improved compared to Vizier as available chip area decreased. Compared to a manually-guided exhaustive or "brute-force" search, P3BO found a better configuration while performing 36% fewer search evaluations.

The design of accelerator hardware for improving AI inference is an active research area. Apple's new [M1 processor](https://www.infoq.com/news/2020/11/apple-tensorflow-acceleration/) includes a neural engine designed to speed up AI computations. Stanford researchers recently published an article in Nature describing a system called [Illusion](https://ee.stanford.edu/news/research-news/01-19-2021/subhasish-mitra-hs-philip-wong-and-mary-wootters-system-can-run-ai) that uses a network of smaller chips to emulate a single larger accelerator. At Google, scientists have also published work on [optimizing chip floorplanning](https://ai.googleblog.com/2020/04/chip-design-with-deep-reinforcement.html), to find the best placement of integrated-circuit components on the physical chip.

> 如果发现译文存在错误或其他需要改进的地方，欢迎到 [掘金翻译计划](https://github.com/xitu/gold-miner) 对译文进行修改并 PR，也可获得相应奖励积分。文章开头的 **本文永久链接** 即为本文在 GitHub 上的 MarkDown 链接。

---

> [掘金翻译计划](https://github.com/xitu/gold-miner) 是一个翻译优质互联网技术文章的社区，文章来源为 [掘金](https://juejin.im) 上的英文分享文章。内容覆盖 [Android](https://github.com/xitu/gold-miner#android)、[iOS](https://github.com/xitu/gold-miner#ios)、[前端](https://github.com/xitu/gold-miner#前端)、[后端](https://github.com/xitu/gold-miner#后端)、[区块链](https://github.com/xitu/gold-miner#区块链)、[产品](https://github.com/xitu/gold-miner#产品)、[设计](https://github.com/xitu/gold-miner#设计)、[人工智能](https://github.com/xitu/gold-miner#人工智能)等领域，想要查看更多优质译文请持续关注 [掘金翻译计划](https://github.com/xitu/gold-miner)、[官方微博](http://weibo.com/juejinfanyi)、[知乎专栏](https://zhuanlan.zhihu.com/juejinfanyi)。
