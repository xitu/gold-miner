> * åŸæ–‡åœ°å€ï¼š[Representing music with Word2vec?](https://towardsdatascience.com/representing-music-with-word2vec-c3c503176d52)
> * åŸæ–‡ä½œè€…ï¼š[Dorien Herremans](https://medium.com/@dorien.herremans)
> * è¯‘æ–‡å‡ºè‡ªï¼š[æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner)
> * æœ¬æ–‡æ°¸ä¹…é“¾æ¥ï¼š[https://github.com/xitu/gold-miner/blob/master/TODO1/representing-music-with-word2vec.md](https://github.com/xitu/gold-miner/blob/master/TODO1/representing-music-with-word2vec.md)
> * è¯‘è€…ï¼š
> * æ ¡å¯¹è€…ï¼š

![](https://cdn-images-1.medium.com/max/1600/0*5vTITPYMya0GrTaN)

# Representing music with Word2vec?

Machine learning algorithms have transformed the field of vision and NLP. But what about music? These last few years, the field of music information retrieval (MIR) has been experiencing a rapid growth. We will be looking at how some of these techniques from NLP can be ported to the field of music. In a recent paper byÂ [Chuan, Agres, & Herremans (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1), they explore how a popular technique from NLP, namely word2vec, can be used to represent polyphonic music. Let's dive into how this was done...

## Word2vec

Word embedding models make it possible for us to represent words in a meaningful way such that machine learning models can more easily process them. They allow us to represent words by a vector that represents semantic meaning. Word2vec is a popular vector embedding model developed by Mikolov et al. (2013), that can create semantic vector spaces in a very efficient manner.

The essence of word2vec is a simple one-layer neural network, built in two possible ways: 1) using continuous bag-of-words (CBOW); or 2) using a skip-gram architecture. Both architectures are quite efficient and can be trained relatively quickly. In this study, we use a skip-gram model, as Mikolov et al. (2013) has hinted at them being more efficient for smaller datasets. Skip-gram architectures take a current word w_t (input layer) and try to predict the surrounding words in the context window (output layer):

![](https://cdn-images-1.medium.com/max/1600/0*sl2WQJQUaD6WoU-w.png)

Figure fromÂ [Chuan et al (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1). Illustration of a wordÂ tÂ and its surrounding contextÂ window.

There is some confusion about how a skip-gram architecture looks like due to someÂ [popular images](https://cdn-images-1.medium.com/max/800/1*SR6l59udY05_bUICAjb6-w.png)Â floating around the internet. The network output does not consist of multiple words, but of one single word from the context window. How can it learn to represent the entire context window? While training the network, we use sampled pairs, consisting of the input word with a random word from the context window.

The traditional training objective of this type of network includes a softmax function to calculate ğ‘(ğ‘¤_{ğ‘¡+ğ‘–}|ğ‘¤_ğ‘¡), whose gradient is expensive to calculate. Luckily, techniques such as noise contrastive estimation (Gutmann & HyvÃ¤rine, 2012) and negative sampling (Mikolov et al, 2013b) offer a solution. We used negative sampling to basically define a new objective: maximise the probability of real words and minimise that of noise samples. A simple binary logistic regression classifies noise samples from real words.

Once a word2vec model is trained, the weights of the hidden layer basically represent the learned, multidimensional, embeddings.

## Music asÂ words?

Music and language are intrinsically related. Both consist of a series of sequential events that follow a set of grammatical rules. More importantly, they both create expectation. Imagine that I say: "I am going to the pizzeria to buy aÂ ...". This generates a clear expectation... pizza. Now imagine I hum you the melody line of Happy Birthday, but I stop just before the last note... Just like a sentence, melodies generate expectations. So much expectatio, that it can be measured by EEGs as, for instance, N400 event-related potentials in the brain (Besson & SchÃ¶n, 2002).

Given the similarity between language and words, let's see if a popular model for language can be used as a meaningful representation of music. To convert a midi file to 'language', we define 'slices' of music (which will be our equivalent to words). Each musical piece in our dataset is segmented into equal duration, non-overlapping, slices of a beat long. The duration of a beat can differ for each piece and is estimated byÂ [MIDI toolbox](about:invalid#zSoyz). For each of these slices, we keep a list of all pitch classes, i.e., pitches without octave information.

The figure below shows an example of how slices are determined for the first bars of Chopin's Mazurka Op. 67 â„–4. A beat is a quarter note long here.

![](https://cdn-images-1.medium.com/max/1600/0*Ho_dJEmlWHsAjLow.png)

Figure fromÂ [Chuan et al (2018)â€Š](http://link.springer.com/article/10.1007/s00521-018-3923-1)---â€ŠCreating words from slices ofÂ music

## Word2vec learns tonalityâ€Š---â€Šdistributional semantics hypothesis forÂ music

In language, the distributional semantics hypothesis drives the motivation behind vector embeddings. It states that "words that appear in the same contexts tend to have similar meanings" (Harris, 1954). Translated to vector spaces, this means that these words will be geometrically close to each other. Let's discover if the word2vec model learns a similar representation for music.

### Dataset

Chuan et al. use aÂ [MIDI dataset](https://www.reddit.com/r/datasets/comments/3akhxy/the_largest_midi_collection_on_the_internet)Â that contains a mix of eight different genres (from classical to metal). From a total of 130,000 pieces, only 23,178 pieces were selected based on the presence of a genre label. Within these pieces, there were 4,076 unique slices

### Hyperparameters

The model was trained using only the 500 most occurring slices (or words), a dummy word was used to replace the others. This procedure augments the accuracy of the model as more information (occurrences) is available of the included words. Other hyperparameters include a learning rate of 0.1, skip window size of 4, number of training steps (1,000,000), and 256 as the size of the embeddings.

### Chords

To evaluate whether semantic meaning of the musical slices is captured by the model, let's look at chords.

In the slice vocabulary, all of the slices containing triads were identified. These were then labeled with their scale degree in Roman numerals (as often done in music theory). For instance, in the key of C, the chord C is I, the G chord on the other hand is represented as V. Cosine distance was then used to calculate how far chords of different scale degrees were from each other in the embedding.

The Cosine distance Ds(A, B) between two non-zero vectors A and B, in anÂ *n-*dimensional space,is calculated as:

Dğ‘(A,B)=1-cos(ğœƒ)=1-Dğ‘ (A,B)

Whereby ğœƒ is the angle between A and B, and Ds is the cosine similarity:

![](https://cdn-images-1.medium.com/max/1600/1*QgZYudn4WhqfTVk0cQPgsw.png)

From a music theory perspective, the 'tonal' distance between a I chord and V should be smaller than say, a I chord and III. The figure below shows the distances between a C major triad and other chords.

![](https://cdn-images-1.medium.com/max/1600/1*Rmfm-Tt8rF_g_tRE8pgABA.png)

Figure fromÂ [Chuan et al (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1)â€Š---â€ŠCosine distance between triads and the tonic chord = C majorÂ triad.

The distance between a I triad to V, IV, and vi are smaller! This corresponds to how they are perceived as 'tonally closer' in music theory, and indicates that the word2vec model learns meaningful relationships between our slices.

*It seems that the cosine distance between chords in word2vec space reflects the functional roles of chords in music theory!*

### Keys

Looking at Bach's Well-Tempered Clavier (WTC)'s 24 preludes, which contain a piece in each of the 24 keys (major and minor), we can study if the new embedding space captured information about key.

The augment the dataset, each of the pieces was transposed to each of the other major or minor keys (depending on the original key), this resulted in 12 versions of each piece. The slices of each of these keys were mapped onto the previously trained vector space and clustered using k-means, such that we get a centroid for each piece in the new dataset. By transposing the pieces to each key, we make sure that the cosine distance between centroids is only affected by 1 element: key.

The resulting cosine distances between each centroid of pieces in different keys are shown in the figure below. As expected, fifths apart are tonally close and are represented as the darker areas next to the diagonal. Tonally far apart keys (e.g. F and F#) have an orange colour, which confirms our hypothesis that the word2vec space reflects tonal distances between keys!

![](https://cdn-images-1.medium.com/max/1600/0*TdjQRpqCOLu6ilBf.png)

Figure fromÂ [Chuan et al (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1)--- similarity matrix based on cosine distance between pairs of preludes in different keys.

### Analogy

One of the striking examples of word2vec is theÂ [image](https://www.distilled.net/uploads/word2vec_chart.jpg)Â that shows translations between king â†’ queen, and man â†’ women, in the vector space (Mikolov et al., 2013c). This shows that meaning can be brought forward through a vector translation. Does this work for music too?

We first detect the chords from polyphonic slices, and look at a chord-pair vectors, going from C major to G major (I-V). The angle between different I-V vectors is shown to be very similar (see figure on the right), and can even be thought of as a multidimensional circle of fifths. This again confirms that the concept of analogy may be present inmusical word2vec spaces, although more investigation is needed to uncover clearer examples.

![](https://cdn-images-1.medium.com/max/1600/0*qUbokC9N7ZEmV3js.png)

Figure fromÂ [Chuan et al (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1)â€Š---â€Šangle between chord-pair vectors.

### Other applicationsâ€Š---â€Šmusic generation?

Chuan et al. (2018) briefly look at how the model can be used to replace slices of music to form new music. They indicate that this is just a preliminary test, but the system could be used as a representation method in a more comprehensive system, e.g. LSTM. More details are given in the scientific paper, but the figure below gives an impression of the result.

![](https://cdn-images-1.medium.com/max/1600/0*MTsizhLreNTZ9UC6.png)

Figure fromÂ [Chuan et al (2018)](http://link.springer.com/article/10.1007/s00521-018-3923-1)â€Š---â€ŠReplacing slices with geometrically closeÂ slices.

## Conclusion

Chuan, Agres, and Herremans (2018) built a word2vec model that captures tonal properties of polyphonic music, without ever feeding the actual notes into the model. The article shows convincing evidence that information about chords and keys can be found in the novel embeddings, so to answer the question i the title: Yes we can represent polyphonic music with word2vec! Now the road is open for embedding this representation into other models that also capture time aspects of music.

## References

- Besson M, SchÃ¶n D (2001) Comparison between language and music. Ann N Y Acad Sci 930(1):232--258.
- Chuan, C. H., Agres, K., & Herremans, D. (2018). From context to concept: exploring semantic relationships in music with word2vec.Â *Neural Computing and Applicationsâ€Š---â€ŠSpecial issue on Deep Learning for Music and Audio*, 1--14.Â [Arxiv preprint](https://arxiv.org/abs/1811.12408).
- Gutmann MU, HyvÃ¤rinen A (2012) Noise-contrastive estimation of unnormalized statistical models, with applications to natural image statistics. J Mach Learn Res 13(Feb):307--361
- Harris ZS (1954) Distributional structure. Word 10(2--3):146--162.
- Mikolov, T., Chen, K., Corrado, G., & Dean, J. (2013). Efficient estimation of word representations in vector space.Â *arXiv preprint arXiv:1301.3781.*
- Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J (2013b) Distributed representations of words and phrases and their compositionality. In: Proceedings of advances in neural information processing systems (NIPS), pp 3111--3119
- Mikolov T, Yih Wt, Zweig G (2013c) Linguistic regularities in continuous space word representations. In: Proceedings of the 2013 conference of the North American chapter of the association for computational linguistics: human language technologies, pp 746--751

> å¦‚æœå‘ç°è¯‘æ–‡å­˜åœ¨é”™è¯¯æˆ–å…¶ä»–éœ€è¦æ”¹è¿›çš„åœ°æ–¹ï¼Œæ¬¢è¿åˆ° [æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner) å¯¹è¯‘æ–‡è¿›è¡Œä¿®æ”¹å¹¶ PRï¼Œä¹Ÿå¯è·å¾—ç›¸åº”å¥–åŠ±ç§¯åˆ†ã€‚æ–‡ç« å¼€å¤´çš„ **æœ¬æ–‡æ°¸ä¹…é“¾æ¥** å³ä¸ºæœ¬æ–‡åœ¨ GitHub ä¸Šçš„ MarkDown é“¾æ¥ã€‚

---

> [æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner) æ˜¯ä¸€ä¸ªç¿»è¯‘ä¼˜è´¨äº’è”ç½‘æŠ€æœ¯æ–‡ç« çš„ç¤¾åŒºï¼Œæ–‡ç« æ¥æºä¸º [æ˜é‡‘](https://juejin.im) ä¸Šçš„è‹±æ–‡åˆ†äº«æ–‡ç« ã€‚å†…å®¹è¦†ç›– [Android](https://github.com/xitu/gold-miner#android)ã€[iOS](https://github.com/xitu/gold-miner#ios)ã€[å‰ç«¯](https://github.com/xitu/gold-miner#å‰ç«¯)ã€[åç«¯](https://github.com/xitu/gold-miner#åç«¯)ã€[åŒºå—é“¾](https://github.com/xitu/gold-miner#åŒºå—é“¾)ã€[äº§å“](https://github.com/xitu/gold-miner#äº§å“)ã€[è®¾è®¡](https://github.com/xitu/gold-miner#è®¾è®¡)ã€[äººå·¥æ™ºèƒ½](https://github.com/xitu/gold-miner#äººå·¥æ™ºèƒ½)ç­‰é¢†åŸŸï¼Œæƒ³è¦æŸ¥çœ‹æ›´å¤šä¼˜è´¨è¯‘æ–‡è¯·æŒç»­å…³æ³¨ [æ˜é‡‘ç¿»è¯‘è®¡åˆ’](https://github.com/xitu/gold-miner)ã€[å®˜æ–¹å¾®åš](http://weibo.com/juejinfanyi)ã€[çŸ¥ä¹ä¸“æ ](https://zhuanlan.zhihu.com/juejinfanyi)ã€‚
